{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss =  0.05880711382110921\n",
      "Validation loss =  0.0772075373809868\n",
      "Train torch loss =  tensor(0.6023, grad_fn=<NllLossBackward>)\n",
      "Validation torch loss =  tensor(0.6159, grad_fn=<NllLossBackward>)\n",
      "Predictions our network: [1 1 0 1 0 1 1 0 0 1 0 0 2 1 2 0 1 2 0 1 2 0 2 1 1 1 2 1 0 2]\n",
      "Predictions with torch: [1 1 0 1 0 2 1 0 0 1 0 0 2 1 2 0 1 2 0 1 2 0 2 1 1 1 1 1 0 2]\n",
      "Validation accuracy: 0.9666666666666667\n",
      "Validation torch accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "def xavier_init(size, gain = 1.0):\n",
    "    \"\"\"\n",
    "    Xavier initialization of network weights.\n",
    "\n",
    "    Arguments:\n",
    "        - size {tuple} -- size of the network to initialise.\n",
    "        - gain {float} -- gain for the Xavier initialisation.\n",
    "\n",
    "    Returns:\n",
    "        {np.ndarray} -- values of the weights.\n",
    "    \"\"\"\n",
    "    low = -gain * np.sqrt(6.0 / np.sum(size))\n",
    "    high = gain * np.sqrt(6.0 / np.sum(size))\n",
    "    return np.random.uniform(low=low, high=high, size=size)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Abstract layer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def backward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_params(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MSELossLayer(Layer):\n",
    "    \"\"\"\n",
    "    MSELossLayer: Computes mean-squared error between y_pred and y_target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache_current = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse(y_pred, y_target):\n",
    "        return np.mean((y_pred - y_target) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse_grad(y_pred, y_target):\n",
    "        return 2 * (y_pred - y_target) / len(y_pred)\n",
    "\n",
    "    def forward(self, y_pred, y_target):\n",
    "        self._cache_current = y_pred, y_target\n",
    "        return self._mse(y_pred, y_target)\n",
    "\n",
    "    def backward(self):\n",
    "        return self._mse_grad(*self._cache_current)\n",
    "\n",
    "\n",
    "class CrossEntropyLossLayer(Layer):\n",
    "    \"\"\"\n",
    "    CrossEntropyLossLayer: Computes the softmax followed by the negative \n",
    "    log-likelihood loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache_current = None\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        numer = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "        denom = numer.sum(axis=1, keepdims=True)\n",
    "        return numer / denom\n",
    "\n",
    "    def forward(self, inputs, y_target):\n",
    "        assert len(inputs) == len(y_target)\n",
    "        n_obs = len(y_target)\n",
    "        probs = self.softmax(inputs)\n",
    "        self._cache_current = y_target, probs\n",
    "\n",
    "        out = -1 / n_obs * np.sum(y_target * np.log(probs))\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        y_target, probs = self._cache_current\n",
    "        n_obs = len(y_target)\n",
    "        return -1 / n_obs * (y_target - probs)\n",
    "\n",
    "\n",
    "class SigmoidLayer(Layer):\n",
    "    \"\"\"\n",
    "    SigmoidLayer: Applies sigmoid function elementwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Constructor of the Sigmoid layer.\n",
    "        \"\"\"\n",
    "        self._cache_current = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Performs forward pass through the Sigmoid layer.\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #store the input in the cache to later use in backward pass\n",
    "        self._cache_current = x\n",
    "        #return x passed through the sigmoid function\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with repect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #retrieve input from the cache\n",
    "        x = self._cache_current\n",
    "        #compute sigmoid derivative of output of previous linear layer (input of activation function)\n",
    "        sigmoid_derivative = np.exp(-x)/(1+np.exp(-x))**2\n",
    "        \n",
    "        #check whether same shape\n",
    "        assert grad_z.shape == sigmoid_derivative.shape\n",
    "        \n",
    "        #do element wise multiplication of gradient wrt to output of activation function and sigmoid derivative \n",
    "        return np.multiply(grad_z, sigmoid_derivative)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class ReluLayer(Layer):\n",
    "    \"\"\"\n",
    "    ReluLayer: Applies Relu function elementwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor of the Relu layer.\n",
    "        \"\"\"\n",
    "        self._cache_current = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Performs forward pass through the Relu layer.\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #store the input in the cache to later use in backward pass\n",
    "        self._cache_current = x\n",
    "        #return x passed through the relu function\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with repect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #retrieve input from the cache\n",
    "        x = self._cache_current\n",
    "        #compute relu derivative of output of previous linear layer (input of activation function)\n",
    "        relu_derivative = (x > 0) * 1\n",
    "        \n",
    "        #check whether same shape\n",
    "        assert grad_z.shape == relu_derivative.shape\n",
    "        \n",
    "        #do element wise multiplication of gradient wrt to output of activation function and relu derivative \n",
    "        return np.multiply(grad_z, relu_derivative)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    \"\"\"\n",
    "    LinearLayer: Performs affine transformation of input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        Constructor of the linear layer.\n",
    "\n",
    "        Arguments:\n",
    "            - n_in {int} -- Number (or dimension) of inputs.\n",
    "            - n_out {int} -- Number (or dimension) of outputs.\n",
    "        \"\"\"\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        # initialize weights of linear layer with Xavier Glorot initialization (gain of 1)\n",
    "        self._W = xavier_init((self.n_in, self.n_out), 1.0)\n",
    "        # initialize biases with zeros, because we don’t want the neurons to start out with a bias\n",
    "        self._b = np.zeros((1,self.n_out))\n",
    "\n",
    "        self._cache_current = None\n",
    "        self._grad_W_current = None\n",
    "        self._grad_b_current = None\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the layer (i.e. returns Wx + b).\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #retrieve input from the cache\n",
    "        self._cache_current = x\n",
    "        \n",
    "        #check whether same shape\n",
    "        assert (x.shape[1] == self._W.shape[0])\n",
    "        \n",
    "        # calculate output of linear layer and return\n",
    "        z = x @ self._W + np.repeat(self._b,x.shape[0],axis=0)\n",
    "        \n",
    "        return z\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        # check whether same shape and multiplications can be performed\n",
    "        \n",
    "        assert (np.transpose(self._cache_current).shape[1] == grad_z.shape[0]) & (np.ones((1,self._cache_current.shape[0])).shape[1] == grad_z.shape[0]) & (grad_z.shape[1] == np.transpose(self._W).shape[0])\n",
    "        \n",
    "        # calculate loss gradient wrt to weights: input_transpose * grad_z\n",
    "        self._grad_W_current = np.transpose(self._cache_current) @ grad_z\n",
    "        # calculate loss gradient wrt to bias: column vector of ones * grad_z\n",
    "        self._grad_b_current = np.ones((1,self._cache_current.shape[0])) @ grad_z\n",
    "        \n",
    "        #return gradient with respect to the inputs of the layer: grad_z * w_transpose\n",
    "        return grad_z @ np.transpose(self._W)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent with given learning rate on the\n",
    "        layer's parameters using currently stored gradients.\n",
    "\n",
    "        Arguments:\n",
    "            learning_rate {float} -- Learning rate of update step.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #update weights and biases \n",
    "        self._W += - learning_rate * self._grad_W_current\n",
    "        self._b += - learning_rate * self._grad_b_current\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class MultiLayerNetwork(object):\n",
    "    \"\"\"\n",
    "    MultiLayerNetwork: A network consisting of stacked linear layers and\n",
    "    activation functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, neurons, activations):\n",
    "        \"\"\"\n",
    "        Constructor of the multi layer network.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dim {int} -- Number of features in the input (excluding \n",
    "                the batch dimension).\n",
    "            - neurons {list} -- Number of neurons in each linear layer \n",
    "                represented as a list. The length of the list determines the \n",
    "                number of linear layers.\n",
    "            - activations {list} -- List of the activation functions to apply \n",
    "                to the output of each linear layer.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.neurons = neurons\n",
    "        self.activations = activations\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #stack all layers in one ndarray \n",
    "        layers = np.ndarray((len(self.neurons)*2),dtype=np.object)\n",
    "        n_in = self.input_dim\n",
    "        for i in range(len(self.neurons)):\n",
    "            layers[2*i] = LinearLayer(n_in,self.neurons[i])\n",
    "            n_in = self.neurons[i]\n",
    "            if (self.activations[i] == 'relu'):\n",
    "                layers[2*i+1] = ReluLayer()\n",
    "            elif (self.activations[i] == 'sigmoid'):\n",
    "                layers[2*i+1] = SigmoidLayer()\n",
    "            else:\n",
    "                layers[2*i+1] = 'identity'\n",
    "            \n",
    "        self._layers = layers\n",
    "                \n",
    "            \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size,\n",
    "                #_neurons_in_final_layer)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #pass through all layers\n",
    "        z_temp = x\n",
    "        for i in range(len(self._layers)):\n",
    "            #check whether identity layer and no activation function should be called\n",
    "            if not self._layers[i] == 'identity':\n",
    "                z_temp = self._layers[i].forward(z_temp)\n",
    "        return z_temp \n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Performs backward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (1,\n",
    "                #_neurons_in_final_layer).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with repect to layer\n",
    "                input, of shape (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #backwards pass through all layers\n",
    "        grad_z_temp = grad_z\n",
    "        for i in range(len(self._layers)-1,-1,-1):\n",
    "            #check whether identity layer and no activation function should be called\n",
    "            if not self._layers[i] == 'identity':\n",
    "                grad_z_temp = self._layers[i].backward(grad_z_temp)\n",
    "        return grad_z_temp\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent with given learning rate on the\n",
    "        parameters of all layers using currently stored gradients.\n",
    "\n",
    "        Arguments:\n",
    "            learning_rate {float} -- Learning rate of update step.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #update all weights and biases in the individual layers\n",
    "        for i in range(len(self._layers)):\n",
    "            #check that only linear layers are called\n",
    "            if isinstance(self._layers[i], LinearLayer):\n",
    "                self._layers[i].update_params(learning_rate)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "def save_network(network, fpath):\n",
    "    \"\"\"\n",
    "    Utility function to pickle `network` at file path `fpath`.\n",
    "    \"\"\"\n",
    "    with open(fpath, \"wb\") as f:\n",
    "        pickle.dump(network, f)\n",
    "\n",
    "\n",
    "def load_network(fpath):\n",
    "    \"\"\"\n",
    "    Utility function to load network found at file path `fpath`.\n",
    "    \"\"\"\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trainer: Object that manages the training of a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        batch_size,\n",
    "        nb_epoch,\n",
    "        learning_rate,\n",
    "        loss_fun,\n",
    "        shuffle_flag,\n",
    "        use_torch\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor of the Trainer.\n",
    "\n",
    "        Arguments:\n",
    "            - network {MultiLayerNetwork} -- MultiLayerNetwork to be trained.\n",
    "            - batch_size {int} -- Training batch size.\n",
    "            - nb_epoch {int} -- Number of training epochs.\n",
    "            - learning_rate {float} -- SGD learning rate to be used in training.\n",
    "            - loss_fun {str} -- Loss function to be used. Possible values: mse,\n",
    "                bce.\n",
    "            - shuffle_flag {bool} -- If True, training data is shuffled before\n",
    "                training.\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fun = loss_fun\n",
    "        self.shuffle_flag = shuffle_flag\n",
    "        self.use_torch = use_torch\n",
    "        if self.use_torch:\n",
    "            self.optimizer = torch.optim.SGD(self.network.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        if loss_fun == 'cross_entropy':\n",
    "            self._loss_layer = CrossEntropyLossLayer()\n",
    "        elif loss_fun == 'mse':\n",
    "            self._loss_layer = MSELossLayer()\n",
    "        #self._loss_layer = CrossEntropyLossLayer() if loss_fun == 'cross_entropy' else self._loss_layer = MSELossLayer()\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Returns shuffled versions of the inputs.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_data_points, #output_neurons).\n",
    "\n",
    "        Returns: \n",
    "            - {np.ndarray} -- shuffled inputs.\n",
    "            - {np.ndarray} -- shuffled_targets.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #randomly shuffle indices and use shuffled indices to get shuffled dataset (both input and target)\n",
    "        idx_list = np.arange(len(input_dataset))\n",
    "        np.random.shuffle(idx_list)\n",
    "        shuffled_inputs  = input_dataset[idx_list]\n",
    "        shuffled_targets  = target_dataset[idx_list]\n",
    "        return shuffled_inputs, shuffled_targets\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def train(self, input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Main training loop. Performs the following steps `nb_epoch` times:\n",
    "            - Shuffles the input data (if `shuffle` is True)\n",
    "            - Splits the dataset into batches of size `batch_size`.\n",
    "            - For each batch:\n",
    "                - Performs forward pass through the network given the current\n",
    "                batch of inputs.\n",
    "                - Computes loss.\n",
    "                - Performs backward pass to compute gradients of loss with\n",
    "                respect to parameters of network.\n",
    "                - Performs one step of gradient descent on the network\n",
    "                parameters.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_training_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_training_data_points, #output_neurons).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        if self.use_torch:\n",
    "            criterion = torch.nn.CrossEntropyLoss()# cross entropy loss\n",
    "        #loop over number of epochs\n",
    "        for i in range(self.nb_epoch):\n",
    "            #check whether data should be shuffled\n",
    "            if self.shuffle_flag == True:\n",
    "                input_dataset, target_dataset = self.shuffle(input_dataset, target_dataset)\n",
    "            #Splits the dataset into batches of size batch_size\n",
    "            input_batches = [input_dataset[i:i + self.batch_size] for i in range(0, len(input_dataset), self.batch_size)]  \n",
    "            target_batches = [target_dataset[i:i + self.batch_size] for i in range(0, len(target_dataset), self.batch_size)] \n",
    "            \n",
    "            #loop over batches\n",
    "            if self.use_torch == False:\n",
    "                for i in range(len(input_batches)):\n",
    "                    y_pred = self.network(input_batches[i])\n",
    "                    self._loss_layer.forward(y_pred, target_batches[i])\n",
    "                    grad_loss = self._loss_layer.backward()\n",
    "                    self.network.backward(grad_loss)\n",
    "                    self.network.update_params(self.learning_rate)\n",
    "            else:\n",
    "                for i in range(len(input_batches)):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    y_pred = self.network.forward(torch.tensor(input_batches[i], dtype=torch.float32))\n",
    "                    labels = torch.tensor(target_batches[i], dtype=torch.float32).argmax(1)\n",
    "                    loss = criterion(y_pred, labels)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def eval_loss(self, input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Function that evaluate the loss function for given data.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_evaluation_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_evaluation_data_points, #output_neurons).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #get current target prediction and return calculated loss (with target data)\n",
    "        if self.use_torch == False:\n",
    "            y_pred = self.network(input_dataset)\n",
    "            return self._loss_layer.forward(y_pred, target_dataset)\n",
    "        else:\n",
    "            y_pred = self.network.forward(torch.tensor(input_dataset, dtype=torch.float32))\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            labels = torch.tensor(target_dataset, dtype=torch.float32).argmax(1)\n",
    "            return criterion(y_pred, labels)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    Preprocessor: Object used to apply \"preprocessing\" operation to datasets.\n",
    "    The object can also be used to revert the changes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initializes the Preprocessor according to the provided dataset.\n",
    "        (Does not modify the dataset.)\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset used to determine the parameters for\n",
    "            the normalization.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #find maximum and minimum in data and set a and b according to range [0,1]\n",
    "        self.maximum = np.amax(data, axis = 0)\n",
    "        self.minimum = np.amin(data, axis = 0)\n",
    "        self.a = np.ones(data.shape[1])\n",
    "        self.b = np.zeros(data.shape[1])\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def apply(self, data):\n",
    "        \"\"\"\n",
    "        Apply the pre-processing operations to the provided dataset.\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} normalized dataset.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #perform min max normalization : Scaling the smallest value to a and largest value to b\n",
    "        normalized_data = self.a + (data - self.minimum) * (self.b-self.a) / (self.maximum - self.minimum)\n",
    "        return normalized_data\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def revert(self, data):\n",
    "        \"\"\"\n",
    "        Revert the pre-processing operations to retreive the original dataset.\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset for which to revert normalization.\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} reverted dataset.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        #revert min max normalization : Scaling a to smallest value and b to largest value\n",
    "        reverted_data = (data - self.a) * (self.maximum - self.minimum) / (self.b-self.a) + self.minimum \n",
    "        return reverted_data\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "        \n",
    "######################## Testing with torch #############################      \n",
    "\n",
    "\n",
    "\n",
    "# The Network class inherits the torch.nn.Module class, which represents a neural network.\n",
    "class Network(torch.nn.Module):\n",
    "\n",
    "    # The class initialisation function. This takes as arguments the dimension of the network's input (i.e. the dimension of the state), and the dimension of the network's output (i.e. the dimension of the action).\n",
    "    def __init__(self):\n",
    "        # Call the initialisation function of the parent class.\n",
    "        super(Network, self).__init__()\n",
    "        self.layer_1 = torch.nn.Linear(in_features=4, out_features=16)    \n",
    "        self.output_layer = torch.nn.Linear(in_features=16, out_features=3)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    # Function which sends some input data through the network and returns the network's output. In this example, a ReLU activation function is used for both hidden layers, but the output layer has no activation function (it is just a linear layer).\n",
    "    def forward(self, input):\n",
    "        layer_1_output = torch.nn.functional.relu(self.layer_1(input))\n",
    "        #layer_2_output = torch.nn.functional.relu(self.layer_2(layer_1_output))\n",
    "        #output = self.output_layer(layer_1_output)\n",
    "        output = self.softmax(self.output_layer(layer_1_output))\n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def example_main():\n",
    "    input_dim = 4\n",
    "    neurons = [16, 3]\n",
    "    activations = [\"relu\", \"identity\"]\n",
    "    net = MultiLayerNetwork(input_dim, neurons, activations)\n",
    "\n",
    "    net_torch = Network()\n",
    "    \n",
    "    dat = np.loadtxt(\"iris.dat\")\n",
    "    np.random.shuffle(dat)\n",
    "\n",
    "    x = dat[:, :4]\n",
    "    y = dat[:, 4:]\n",
    "\n",
    "    split_idx = int(0.8 * len(x))\n",
    "\n",
    "    x_train = x[:split_idx]\n",
    "    y_train = y[:split_idx]\n",
    "    x_val = x[split_idx:]\n",
    "    y_val = y[split_idx:]\n",
    "\n",
    "    prep_input = Preprocessor(x_train)\n",
    "\n",
    "    x_train_pre = prep_input.apply(x_train)\n",
    "    x_val_pre = prep_input.apply(x_val)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        network=net,\n",
    "        batch_size=8,\n",
    "        nb_epoch=1000,\n",
    "        learning_rate=0.01,\n",
    "        loss_fun=\"cross_entropy\",\n",
    "        shuffle_flag=True,\n",
    "        use_torch = False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer_torch = Trainer(\n",
    "        network=net_torch,\n",
    "        batch_size=8,\n",
    "        nb_epoch=1000,\n",
    "        learning_rate=0.01,\n",
    "        loss_fun=\"cross_entropy\",\n",
    "        shuffle_flag=True,\n",
    "        use_torch = True\n",
    "    )\n",
    "\n",
    "    trainer.train(x_train_pre, y_train)\n",
    "    trainer_torch.train(x_train_pre, y_train)\n",
    "    print(\"Train loss = \", trainer.eval_loss(x_train_pre, y_train))\n",
    "    print(\"Validation loss = \", trainer.eval_loss(x_val_pre, y_val))\n",
    "    print(\"Train torch loss = \", trainer_torch.eval_loss(x_train_pre, y_train))\n",
    "    print(\"Validation torch loss = \", trainer_torch.eval_loss(x_val_pre, y_val))\n",
    "\n",
    "    preds = net(x_val_pre).argmax(axis=1).squeeze()\n",
    "    print(\"Predictions our network: {}\".format(preds))\n",
    "    preds_torch = net_torch.forward(torch.tensor(x_val_pre, dtype=torch.float32)).argmax(axis=1).squeeze().numpy()\n",
    "    print(\"Predictions with torch: {}\".format(preds_torch))\n",
    "    targets = y_val.argmax(axis=1).squeeze()\n",
    "    accuracy = (preds == targets).mean()\n",
    "    accuracy_torch = (preds_torch == targets).mean()\n",
    "    print(\"Validation accuracy: {}\".format(accuracy))\n",
    "    print(\"Validation torch accuracy: {}\".format(accuracy_torch))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
