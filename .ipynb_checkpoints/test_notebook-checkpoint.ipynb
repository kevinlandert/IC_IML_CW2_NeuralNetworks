{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "def xavier_init(size, gain = 1.0):\n",
    "    \"\"\"\n",
    "    Xavier initialization of network weights.\n",
    "\n",
    "    Arguments:\n",
    "        - size {tuple} -- size of the network to initialise.\n",
    "        - gain {float} -- gain for the Xavier initialisation.\n",
    "\n",
    "    Returns:\n",
    "        {np.ndarray} -- values of the weights.\n",
    "    \"\"\"\n",
    "    low = -gain * np.sqrt(6.0 / np.sum(size))\n",
    "    high = gain * np.sqrt(6.0 / np.sum(size))\n",
    "    return np.random.uniform(low=low, high=high, size=size)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Abstract layer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def backward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_params(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MSELossLayer(Layer):\n",
    "    \"\"\"\n",
    "    MSELossLayer: Computes mean-squared error between y_pred and y_target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache_current = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse(y_pred, y_target):\n",
    "        return np.mean((y_pred - y_target) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse_grad(y_pred, y_target):\n",
    "        return 2 * (y_pred - y_target) / len(y_pred)\n",
    "\n",
    "    def forward(self, y_pred, y_target):\n",
    "        self._cache_current = y_pred, y_target\n",
    "        return self._mse(y_pred, y_target)\n",
    "\n",
    "    def backward(self):\n",
    "        return self._mse_grad(*self._cache_current)\n",
    "\n",
    "\n",
    "class CrossEntropyLossLayer(Layer):\n",
    "    \"\"\"\n",
    "    CrossEntropyLossLayer: Computes the softmax followed by the negative \n",
    "    log-likelihood loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache_current = None\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        numer = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "        denom = numer.sum(axis=1, keepdims=True)\n",
    "        return numer / denom\n",
    "\n",
    "    def forward(self, inputs, y_target):\n",
    "        assert len(inputs) == len(y_target)\n",
    "        n_obs = len(y_target)\n",
    "        probs = self.softmax(inputs)\n",
    "        self._cache_current = y_target, probs\n",
    "\n",
    "        out = -1 / n_obs * np.sum(y_target * np.log(probs))\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        y_target, probs = self._cache_current\n",
    "        n_obs = len(y_target)\n",
    "        return -1 / n_obs * (y_target - probs)\n",
    "\n",
    "\n",
    "class SigmoidLayer(Layer):\n",
    "    \"\"\"\n",
    "    SigmoidLayer: Applies sigmoid function elementwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Constructor of the Sigmoid layer.\n",
    "        \"\"\"\n",
    "        self._cache_current = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Performs forward pass through the Sigmoid layer.\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._cache_current = x\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with repect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        x = self._cache_current\n",
    "        sigmoid_derivative = np.exp(-x)/(1+np.exp(-x))**2\n",
    "        return np.multiply(grad_z, sigmoid_derivative)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class ReluLayer(Layer):\n",
    "    \"\"\"\n",
    "    ReluLayer: Applies Relu function elementwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor of the Relu layer.\n",
    "        \"\"\"\n",
    "        self._cache_current = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Performs forward pass through the Relu layer.\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._cache_current = x\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with repect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        x = self._cache_current\n",
    "        relu_derivative = (x > 0) * 1\n",
    "        return np.multiply(grad_z, relu_derivative)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    \"\"\"\n",
    "    LinearLayer: Performs affine transformation of input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        Constructor of the linear layer.\n",
    "\n",
    "        Arguments:\n",
    "            - n_in {int} -- Number (or dimension) of inputs.\n",
    "            - n_out {int} -- Number (or dimension) of outputs.\n",
    "        \"\"\"\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._W = xavier_init((self.n_in, self.n_out), 1.0)\n",
    "        self._b = np.zeros((1,self.n_out))\n",
    "\n",
    "        self._cache_current = None\n",
    "        self._grad_W_current = None\n",
    "        self._grad_b_current = None\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the layer (i.e. returns Wx + b).\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        z = x @ self._W + np.repeat(self._b,x.shape[0],axis=0)\n",
    "        self._cache_current = x\n",
    "        return z\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._grad_W_current = np.transpose(self._cache_current) @ grad_z\n",
    "        self._grad_b_current = np.ones((1,self._cache_current.shape[0])) @ grad_z\n",
    "        \n",
    "        return grad_z @ np.transpose(self._W)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent with given learning rate on the\n",
    "        layer's parameters using currently stored gradients.\n",
    "\n",
    "        Arguments:\n",
    "            learning_rate {float} -- Learning rate of update step.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._W += - learning_rate * self._grad_W_current\n",
    "        self._b += - learning_rate * self._grad_b_current\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class MultiLayerNetwork(object):\n",
    "    \"\"\"\n",
    "    MultiLayerNetwork: A network consisting of stacked linear layers and\n",
    "    activation functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, neurons, activations):\n",
    "        \"\"\"\n",
    "        Constructor of the multi layer network.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dim {int} -- Number of features in the input (excluding \n",
    "                the batch dimension).\n",
    "            - neurons {list} -- Number of neurons in each linear layer \n",
    "                represented as a list. The length of the list determines the \n",
    "                number of linear layers.\n",
    "            - activations {list} -- List of the activation functions to apply \n",
    "                to the output of each linear layer.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.neurons = neurons\n",
    "        self.activations = activations\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._layers = None\n",
    "        layers = np.ndarray((len(self.neurons)*2),dtype=np.object)\n",
    "        n_in = self.input_dim\n",
    "        for i in range(len(self.neurons)):\n",
    "            layers[2*i] = LinearLayer(n_in,self.neurons[i])\n",
    "            n_in = self.neurons[i]\n",
    "            if (self.activations[i] == 'relu'):\n",
    "                layers[2*i+1] = ReluLayer()\n",
    "            elif (self.activations[i] == 'sigmoid'):\n",
    "                layers[2*i+1] = SigmoidLayer()\n",
    "            else:\n",
    "                layers[2*i+1] = 'identity'\n",
    "            \n",
    "        self._layers = layers\n",
    "                \n",
    "            \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size,\n",
    "                #_neurons_in_final_layer)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        z_temp = x\n",
    "        for i in range(len(self._layers)):\n",
    "            if not self._layers[i] == 'identity':\n",
    "                z_temp = self._layers[i].forward(z_temp)\n",
    "        return z_temp \n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Performs backward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (1,\n",
    "                #_neurons_in_final_layer).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with repect to layer\n",
    "                input, of shape (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        grad_z_temp = grad_z\n",
    "        for i in range(len(self._layers)-1,-1,-1):\n",
    "            if not self._layers[i] == 'identity':\n",
    "                grad_z_temp = self._layers[i].backward(grad_z_temp)\n",
    "        return grad_z_temp\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent with given learning rate on the\n",
    "        parameters of all layers using currently stored gradients.\n",
    "\n",
    "        Arguments:\n",
    "            learning_rate {float} -- Learning rate of update step.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        for i in range(len(self._layers)):\n",
    "            if isinstance(self._layers[i], LinearLayer):\n",
    "                self._layers[i].update_params(learning_rate)\n",
    "        \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "def save_network(network, fpath):\n",
    "    \"\"\"\n",
    "    Utility function to pickle `network` at file path `fpath`.\n",
    "    \"\"\"\n",
    "    with open(fpath, \"wb\") as f:\n",
    "        pickle.dump(network, f)\n",
    "\n",
    "\n",
    "def load_network(fpath):\n",
    "    \"\"\"\n",
    "    Utility function to load network found at file path `fpath`.\n",
    "    \"\"\"\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trainer: Object that manages the training of a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        batch_size,\n",
    "        nb_epoch,\n",
    "        learning_rate,\n",
    "        loss_fun,\n",
    "        shuffle_flag,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor of the Trainer.\n",
    "\n",
    "        Arguments:\n",
    "            - network {MultiLayerNetwork} -- MultiLayerNetwork to be trained.\n",
    "            - batch_size {int} -- Training batch size.\n",
    "            - nb_epoch {int} -- Number of training epochs.\n",
    "            - learning_rate {float} -- SGD learning rate to be used in training.\n",
    "            - loss_fun {str} -- Loss function to be used. Possible values: mse,\n",
    "                bce.\n",
    "            - shuffle_flag {bool} -- If True, training data is shuffled before\n",
    "                training.\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fun = loss_fun\n",
    "        self.shuffle_flag = shuffle_flag\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        if loss_fun == 'cross_entropy':\n",
    "            self._loss_layer = MSELossLayer()\n",
    "        elif loss_fun == 'mse':\n",
    "            self._loss_layer = MSELossLayer()\n",
    "        #self._loss_layer = CrossEntropyLossLayer() if loss_fun == 'cross_entropy' else self._loss_layer = MSELossLayer()\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Returns shuffled versions of the inputs.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_data_points, #output_neurons).\n",
    "\n",
    "        Returns: \n",
    "            - {np.ndarray} -- shuffled inputs.\n",
    "            - {np.ndarray} -- shuffled_targets.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        idx_list = np.arange(len(input_dataset))\n",
    "        np.random.shuffle(idx_list)\n",
    "        shuffled_inputs  = input_dataset[idx_list]\n",
    "        shuffled_targets  = target_dataset[idx_list]\n",
    "        return shuffled_inputs, shuffled_targets\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def train(self, input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Main training loop. Performs the following steps `nb_epoch` times:\n",
    "            - Shuffles the input data (if `shuffle` is True)\n",
    "            - Splits the dataset into batches of size `batch_size`.\n",
    "            - For each batch:\n",
    "                - Performs forward pass through the network given the current\n",
    "                batch of inputs.\n",
    "                - Computes loss.\n",
    "                - Performs backward pass to compute gradients of loss with\n",
    "                respect to parameters of network.\n",
    "                - Performs one step of gradient descent on the network\n",
    "                parameters.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_training_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_training_data_points, #output_neurons).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        losses = []\n",
    "        \n",
    "        for i in range(self.nb_epoch):\n",
    "            loss = 0\n",
    "            if self.shuffle_flag == True:\n",
    "                input_dataset, target_dataset = self.shuffle(input_dataset, target_dataset)\n",
    "            input_batches = [input_dataset[i:i + self.batch_size] for i in range(0, len(input_dataset), self.batch_size)]  \n",
    "            target_batches = [target_dataset[i:i + self.batch_size] for i in range(0, len(target_dataset), self.batch_size)] \n",
    "            for j in range(len(input_batches)):\n",
    "                y_pred = self.network(input_batches[j])\n",
    "                loss += self._loss_layer.forward(y_pred, target_batches[j])\n",
    "                grad_loss = self._loss_layer.backward()\n",
    "                self.network.backward(grad_loss)\n",
    "                self.network.update_params(self.learning_rate)\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(\"epoch {}, loss: {}\".format((i+1),loss/len(input_batches)))\n",
    "                print('hey')\n",
    "            \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def eval_loss(self, input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Function that evaluate the loss function for given data.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_evaluation_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_evaluation_data_points, #output_neurons).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        y_pred = self.network(input_dataset)\n",
    "        return self._loss_layer.forward(y_pred, target_dataset)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    Preprocessor: Object used to apply \"preprocessing\" operation to datasets.\n",
    "    The object can also be used to revert the changes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initializes the Preprocessor according to the provided dataset.\n",
    "        (Does not modify the dataset.)\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset used to determine the parameters for\n",
    "            the normalization.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self.maximum = np.amax(data, axis = 0)\n",
    "        self.minimum = np.amin(data, axis = 0)\n",
    "        self.a = np.ones(data.shape[1])\n",
    "        self.b = np.zeros(data.shape[1])\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def apply(self, data):\n",
    "        \"\"\"\n",
    "        Apply the pre-processing operations to the provided dataset.\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} normalized dataset.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        normalized_data = self.a + (data - self.minimum) * (self.b-self.a) / (self.maximum - self.minimum)\n",
    "        return normalized_data\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def revert(self, data):\n",
    "        \"\"\"\n",
    "        Revert the pre-processing operations to retreive the original dataset.\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset for which to revert normalization.\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} reverted dataset.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        reverted_data = (data - self.a) * (self.maximum - self.minimum) / (self.b-self.a) + self.minimum \n",
    "        return reverted_data\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "def example_main():\n",
    "    input_dim = 4\n",
    "    neurons = [16, 3]\n",
    "    activations = [\"relu\", \"identity\"]\n",
    "    net = MultiLayerNetwork(input_dim, neurons, activations)\n",
    "\n",
    "    dat = np.loadtxt(\"iris.dat\")\n",
    "    np.random.shuffle(dat)\n",
    "\n",
    "    x = dat[:, :4]\n",
    "    y = dat[:, 4:]\n",
    "\n",
    "    split_idx = int(0.8 * len(x))\n",
    "\n",
    "    x_train = x[:split_idx]\n",
    "    y_train = y[:split_idx]\n",
    "    x_val = x[split_idx:]\n",
    "    y_val = y[split_idx:]\n",
    "\n",
    "    prep_input = Preprocessor(x_train)\n",
    "\n",
    "    x_train_pre = prep_input.apply(x_train)\n",
    "    x_val_pre = prep_input.apply(x_val)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        network=net,\n",
    "        batch_size=8,\n",
    "        nb_epoch=1000,\n",
    "        learning_rate=0.01,\n",
    "        loss_fun=\"cross_entropy\",\n",
    "        shuffle_flag=True,\n",
    "    )\n",
    "\n",
    "    trainer.train(x_train_pre, y_train)\n",
    "    print(\"Train loss = \", trainer.eval_loss(x_train_pre, y_train))\n",
    "    print(\"Validation loss = \", trainer.eval_loss(x_val_pre, y_val))\n",
    "\n",
    "    preds = net(x_val_pre).argmax(axis=1).squeeze()\n",
    "    targets = y_val.argmax(axis=1).squeeze()\n",
    "    accuracy = (preds == targets).mean()\n",
    "    print(\"Validation accuracy: {}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss: 0.06797038132275772\n",
      "hey\n",
      "epoch 200, loss: 0.04955308955569184\n",
      "hey\n",
      "epoch 300, loss: 0.01690710835395898\n",
      "hey\n",
      "epoch 400, loss: 0.0643823495992515\n",
      "hey\n",
      "epoch 500, loss: 0.007734429656029639\n",
      "hey\n",
      "epoch 600, loss: 0.005117228112167005\n",
      "hey\n",
      "epoch 700, loss: 0.02010663739041041\n",
      "hey\n",
      "epoch 800, loss: 0.00442985564816283\n",
      "hey\n",
      "epoch 900, loss: 0.0118384322517949\n",
      "hey\n",
      "epoch 1000, loss: 0.018393753282647345\n",
      "hey\n",
      "Train loss =  0.01932074201263425\n",
      "Validation loss =  0.020647674162371954\n",
      "Validation accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "example_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
