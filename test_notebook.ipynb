{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "def xavier_init(size, gain = 1.0):\n",
    "    \"\"\"\n",
    "    Xavier initialization of network weights.\n",
    "\n",
    "    Arguments:\n",
    "        - size {tuple} -- size of the network to initialise.\n",
    "        - gain {float} -- gain for the Xavier initialisation.\n",
    "\n",
    "    Returns:\n",
    "        {np.ndarray} -- values of the weights.\n",
    "    \"\"\"\n",
    "    low = -gain * np.sqrt(6.0 / np.sum(size))\n",
    "    high = gain * np.sqrt(6.0 / np.sum(size))\n",
    "    return np.random.uniform(low=low, high=high, size=size)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Abstract layer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def backward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_params(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MSELossLayer(Layer):\n",
    "    \"\"\"\n",
    "    MSELossLayer: Computes mean-squared error between y_pred and y_target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache_current = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse(y_pred, y_target):\n",
    "        return np.mean((y_pred - y_target) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse_grad(y_pred, y_target):\n",
    "        return 2 * (y_pred - y_target) / len(y_pred)\n",
    "\n",
    "    def forward(self, y_pred, y_target):\n",
    "        self._cache_current = y_pred, y_target\n",
    "        return self._mse(y_pred, y_target)\n",
    "\n",
    "    def backward(self):\n",
    "        return self._mse_grad(*self._cache_current)\n",
    "\n",
    "\n",
    "class CrossEntropyLossLayer(Layer):\n",
    "    \"\"\"\n",
    "    CrossEntropyLossLayer: Computes the softmax followed by the negative \n",
    "    log-likelihood loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache_current = None\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        numer = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "        denom = numer.sum(axis=1, keepdims=True)\n",
    "        return numer / denom\n",
    "\n",
    "    def forward(self, inputs, y_target):\n",
    "        assert len(inputs) == len(y_target)\n",
    "        n_obs = len(y_target)\n",
    "        probs = self.softmax(inputs)\n",
    "        self._cache_current = y_target, probs\n",
    "\n",
    "        out = -1 / n_obs * np.sum(y_target * np.log(probs))\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        y_target, probs = self._cache_current\n",
    "        n_obs = len(y_target)\n",
    "        return -1 / n_obs * (y_target - probs)\n",
    "\n",
    "\n",
    "class SigmoidLayer(Layer):\n",
    "    \"\"\"\n",
    "    SigmoidLayer: Applies sigmoid function elementwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Constructor of the Sigmoid layer.\n",
    "        \"\"\"\n",
    "        self._cache_current = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Performs forward pass through the Sigmoid layer.\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._cache_current = x\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with repect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        x = self._cache_current\n",
    "        sigmoid_derivative = np.exp(-x)/(1+np.exp(-x))**2\n",
    "        return np.multiply(grad_z, sigmoid_derivative)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class ReluLayer(Layer):\n",
    "    \"\"\"\n",
    "    ReluLayer: Applies Relu function elementwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor of the Relu layer.\n",
    "        \"\"\"\n",
    "        self._cache_current = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Performs forward pass through the Relu layer.\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._cache_current = x\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with repect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        x = self._cache_current\n",
    "        relu_derivative = (x > 0) * 1\n",
    "        return np.multiply(grad_z, relu_derivative)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    \"\"\"\n",
    "    LinearLayer: Performs affine transformation of input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        Constructor of the linear layer.\n",
    "\n",
    "        Arguments:\n",
    "            - n_in {int} -- Number (or dimension) of inputs.\n",
    "            - n_out {int} -- Number (or dimension) of outputs.\n",
    "        \"\"\"\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._W = xavier_init((self.n_in, self.n_out), 1.0)\n",
    "        self._b = np.zeros((1,self.n_out))\n",
    "\n",
    "        self._cache_current = None\n",
    "        self._grad_W_current = None\n",
    "        self._grad_b_current = None\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the layer (i.e. returns Wx + b).\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        z = x @ self._W + np.repeat(self._b,x.shape[0],axis=0)\n",
    "        self._cache_current = x\n",
    "        return z\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._grad_W_current = np.transpose(self._cache_current) @ grad_z\n",
    "        self._grad_b_current = np.ones((1,self._cache_current.shape[0])) @ grad_z\n",
    "        \n",
    "        return grad_z @ np.transpose(self._W)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent with given learning rate on the\n",
    "        layer's parameters using currently stored gradients.\n",
    "\n",
    "        Arguments:\n",
    "            learning_rate {float} -- Learning rate of update step.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._W += - learning_rate * self._grad_W_current\n",
    "        self._b += - learning_rate * self._grad_b_current\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class MultiLayerNetwork(object):\n",
    "    \"\"\"\n",
    "    MultiLayerNetwork: A network consisting of stacked linear layers and\n",
    "    activation functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, neurons, activations):\n",
    "        \"\"\"\n",
    "        Constructor of the multi layer network.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dim {int} -- Number of features in the input (excluding \n",
    "                the batch dimension).\n",
    "            - neurons {list} -- Number of neurons in each linear layer \n",
    "                represented as a list. The length of the list determines the \n",
    "                number of linear layers.\n",
    "            - activations {list} -- List of the activation functions to apply \n",
    "                to the output of each linear layer.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.neurons = neurons\n",
    "        self.activations = activations\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._layers = None\n",
    "        layers = np.ndarray((len(self.neurons)*2),dtype=np.object)\n",
    "        n_in = self.input_dim\n",
    "        for i in range(len(self.neurons)):\n",
    "            layers[2*i] = LinearLayer(n_in,self.neurons[i])\n",
    "            n_in = self.neurons[i]\n",
    "            if (self.activations[i] == 'relu'):\n",
    "                layers[2*i+1] = ReluLayer()\n",
    "            elif (self.activations[i] == 'sigmoid'):\n",
    "                layers[2*i+1] = SigmoidLayer()\n",
    "            else:\n",
    "                layers[2*i+1] = 'identity'\n",
    "            \n",
    "        self._layers = layers\n",
    "                \n",
    "            \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size,\n",
    "                #_neurons_in_final_layer)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        z_temp = x\n",
    "        for i in range(len(self._layers)):\n",
    "            if not self._layers[i] == 'identity':\n",
    "                z_temp = self._layers[i].forward(z_temp)\n",
    "        return z_temp \n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Performs backward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (1,\n",
    "                #_neurons_in_final_layer).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with repect to layer\n",
    "                input, of shape (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent with given learning rate on the\n",
    "        parameters of all layers using currently stored gradients.\n",
    "\n",
    "        Arguments:\n",
    "            learning_rate {float} -- Learning rate of update step.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "def save_network(network, fpath):\n",
    "    \"\"\"\n",
    "    Utility function to pickle `network` at file path `fpath`.\n",
    "    \"\"\"\n",
    "    with open(fpath, \"wb\") as f:\n",
    "        pickle.dump(network, f)\n",
    "\n",
    "\n",
    "def load_network(fpath):\n",
    "    \"\"\"\n",
    "    Utility function to load network found at file path `fpath`.\n",
    "    \"\"\"\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trainer: Object that manages the training of a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        batch_size,\n",
    "        nb_epoch,\n",
    "        learning_rate,\n",
    "        loss_fun,\n",
    "        shuffle_flag,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor of the Trainer.\n",
    "\n",
    "        Arguments:\n",
    "            - network {MultiLayerNetwork} -- MultiLayerNetwork to be trained.\n",
    "            - batch_size {int} -- Training batch size.\n",
    "            - nb_epoch {int} -- Number of training epochs.\n",
    "            - learning_rate {float} -- SGD learning rate to be used in training.\n",
    "            - loss_fun {str} -- Loss function to be used. Possible values: mse,\n",
    "                bce.\n",
    "            - shuffle_flag {bool} -- If True, training data is shuffled before\n",
    "                training.\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fun = loss_fun\n",
    "        self.shuffle_flag = shuffle_flag\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._loss_layer = None\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Returns shuffled versions of the inputs.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_data_points, #output_neurons).\n",
    "\n",
    "        Returns: \n",
    "            - {np.ndarray} -- shuffled inputs.\n",
    "            - {np.ndarray} -- shuffled_targets.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def train(self, input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Main training loop. Performs the following steps `nb_epoch` times:\n",
    "            - Shuffles the input data (if `shuffle` is True)\n",
    "            - Splits the dataset into batches of size `batch_size`.\n",
    "            - For each batch:\n",
    "                - Performs forward pass through the network given the current\n",
    "                batch of inputs.\n",
    "                - Computes loss.\n",
    "                - Performs backward pass to compute gradients of loss with\n",
    "                respect to parameters of network.\n",
    "                - Performs one step of gradient descent on the network\n",
    "                parameters.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_training_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_training_data_points, #output_neurons).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def eval_loss(self, input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Function that evaluate the loss function for given data.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_evaluation_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_evaluation_data_points, #output_neurons).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    Preprocessor: Object used to apply \"preprocessing\" operation to datasets.\n",
    "    The object can also be used to revert the changes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initializes the Preprocessor according to the provided dataset.\n",
    "        (Does not modify the dataset.)\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset used to determine the parameters for\n",
    "            the normalization.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def apply(self, data):\n",
    "        \"\"\"\n",
    "        Apply the pre-processing operations to the provided dataset.\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} normalized dataset.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def revert(self, data):\n",
    "        \"\"\"\n",
    "        Revert the pre-processing operations to retreive the original dataset.\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset for which to revert normalization.\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} reverted dataset.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "\n",
    "def example_main():\n",
    "    input_dim = 4\n",
    "    neurons = [16, 3]\n",
    "    activations = [\"relu\", \"identity\"]\n",
    "    net = MultiLayerNetwork(input_dim, neurons, activations)\n",
    "\n",
    "    dat = np.loadtxt(\"iris.dat\")\n",
    "    np.random.shuffle(dat)\n",
    "\n",
    "    x = dat[:, :4]\n",
    "    y = dat[:, 4:]\n",
    "\n",
    "    split_idx = int(0.8 * len(x))\n",
    "\n",
    "    x_train = x[:split_idx]\n",
    "    y_train = y[:split_idx]\n",
    "    x_val = x[split_idx:]\n",
    "    y_val = y[split_idx:]\n",
    "\n",
    "    prep_input = Preprocessor(x_train)\n",
    "\n",
    "    x_train_pre = prep_input.apply(x_train)\n",
    "    x_val_pre = prep_input.apply(x_val)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        network=net,\n",
    "        batch_size=8,\n",
    "        nb_epoch=1000,\n",
    "        learning_rate=0.01,\n",
    "        loss_fun=\"cross_entropy\",\n",
    "        shuffle_flag=True,\n",
    "    )\n",
    "\n",
    "    trainer.train(x_train_pre, y_train)\n",
    "    print(\"Train loss = \", trainer.eval_loss(x_train_pre, y_train))\n",
    "    print(\"Validation loss = \", trainer.eval_loss(x_val_pre, y_val))\n",
    "\n",
    "    preds = net(x_val_pre).argmax(axis=1).squeeze()\n",
    "    targets = y_val.argmax(axis=1).squeeze()\n",
    "    accuracy = (preds == targets).mean()\n",
    "    print(\"Validation accuracy: {}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.96048118, -5.80923584,  0.38211799],\n",
       "       [-1.35762615, -3.21433041,  0.11751725],\n",
       "       [-2.51118204, -4.92270456,  0.27031363],\n",
       "       [-1.3645073 , -3.32127392,  0.09970485],\n",
       "       [-3.68809404, -7.65856176,  0.45034217],\n",
       "       [-2.67185189, -5.61818054,  0.25893161],\n",
       "       [-2.92171359, -6.44846277,  0.35324317],\n",
       "       [-3.54057708, -7.87509117,  0.57586174],\n",
       "       [-2.859165  , -5.98089768,  0.4171091 ],\n",
       "       [-1.67697599, -3.53317435,  0.06324448],\n",
       "       [-3.34537901, -7.37993615,  0.54153924],\n",
       "       [-1.74706326, -3.32887912,  0.06980666],\n",
       "       [-3.59772102, -7.72955595,  0.55179099],\n",
       "       [-1.40778606, -3.25473992,  0.04188119],\n",
       "       [-2.40820201, -5.24116787,  0.28356749],\n",
       "       [-2.49590696, -5.2229307 ,  0.28676526],\n",
       "       [-1.26131817, -3.07238342,  0.09147371],\n",
       "       [-2.99563699, -6.27998658,  0.32751365],\n",
       "       [-1.6211575 , -3.80540418,  0.09322692],\n",
       "       [-3.48051539, -6.32335509,  0.40779313],\n",
       "       [-3.03503482, -6.26104176,  0.36765217],\n",
       "       [-3.05057278, -6.5038268 ,  0.35223703],\n",
       "       [-2.60717952, -4.98079849,  0.35691276],\n",
       "       [-3.73063998, -7.69664849,  0.50314118],\n",
       "       [-1.33530859, -3.33156764,  0.04573245],\n",
       "       [-1.66188878, -3.43933442,  0.1332684 ],\n",
       "       [-1.30597557, -3.11691528,  0.08705375],\n",
       "       [-3.14241527, -6.46163431,  0.44098695],\n",
       "       [-2.7268829 , -6.03057867,  0.32914787],\n",
       "       [-1.39232738, -3.19991434,  0.10027355],\n",
       "       [-1.34857818, -3.20368302,  0.09530328],\n",
       "       [-2.15462779, -4.50297591,  0.2232572 ],\n",
       "       [-2.7938814 , -6.22952374,  0.51259758],\n",
       "       [-2.99200885, -5.83288446,  0.34683601],\n",
       "       [-3.25194621, -6.3019874 ,  0.35620139],\n",
       "       [-2.78541854, -5.84952897,  0.36208005],\n",
       "       [-2.58368702, -5.31979929,  0.31158631],\n",
       "       [-1.39600715, -3.43963014,  0.09840992],\n",
       "       [-2.62078148, -5.48167199,  0.35502607],\n",
       "       [-3.20645078, -7.08224283,  0.47961683],\n",
       "       [-3.52017611, -6.84742143,  0.44896814],\n",
       "       [-2.48776731, -5.32602154,  0.35818299],\n",
       "       [-2.9365462 , -5.95585995,  0.30494106],\n",
       "       [-2.39708192, -5.64168042,  0.29786834],\n",
       "       [-3.13826028, -6.36134789,  0.44313774],\n",
       "       [-2.72916616, -5.77630899,  0.30197702],\n",
       "       [-2.80247014, -5.71530102,  0.32515559],\n",
       "       [-3.39723758, -6.33059169,  0.36717889],\n",
       "       [-2.30636825, -5.10534726,  0.26508484],\n",
       "       [-3.81047743, -7.67021517,  0.542491  ],\n",
       "       [-1.35992729, -3.34298035,  0.06224993],\n",
       "       [-2.12151138, -4.74123452,  0.25842522],\n",
       "       [-1.44488615, -3.34563001,  0.12134682],\n",
       "       [-2.82428952, -6.17739985,  0.38504699],\n",
       "       [-1.61736569, -3.61974863,  0.04239468],\n",
       "       [-3.09030043, -6.36445902,  0.46643608],\n",
       "       [-1.39294523, -3.60242571,  0.11953877],\n",
       "       [-1.3264847 , -3.21279975,  0.12891023],\n",
       "       [-2.44564374, -5.40685935,  0.31336485],\n",
       "       [-2.92498363, -6.16399581,  0.28739143],\n",
       "       [-1.6531992 , -3.5455126 ,  0.12115249],\n",
       "       [-3.00031883, -6.03345729,  0.34253444],\n",
       "       [-3.40084827, -6.65345208,  0.43555947],\n",
       "       [-2.36395467, -5.15218872,  0.27286457],\n",
       "       [-3.67324399, -8.02692285,  0.58634722],\n",
       "       [-2.9965111 , -5.93698965,  0.33104282],\n",
       "       [-3.4473146 , -7.1008061 ,  0.45764005],\n",
       "       [-3.35837744, -6.73047112,  0.40438373],\n",
       "       [-1.62663776, -3.52227551,  0.17000039],\n",
       "       [-3.78620446, -7.15872234,  0.44265429],\n",
       "       [-1.55001008, -3.29662211,  0.11292136],\n",
       "       [-2.93742032, -5.61286302,  0.30847023],\n",
       "       [-1.04968376, -2.74961059,  0.10362972],\n",
       "       [-1.34893657, -3.32050859,  0.10540134],\n",
       "       [-3.54092277, -6.49686148,  0.46574537],\n",
       "       [-3.39546107, -6.16459351,  0.38765171],\n",
       "       [-1.45508073, -3.26574098,  0.14028151],\n",
       "       [-2.69364299, -5.22828732,  0.34635416],\n",
       "       [-2.73551185, -5.38297823,  0.33873505],\n",
       "       [-1.21803941, -3.13891741,  0.14929737],\n",
       "       [-1.24985702, -2.98714634,  0.0718312 ],\n",
       "       [-2.61796053, -5.35500706,  0.30485356],\n",
       "       [-3.24249924, -5.90140032,  0.3559373 ],\n",
       "       [-3.3662275 , -6.61442553,  0.45593462],\n",
       "       [-1.53685253, -3.37044647,  0.1160464 ],\n",
       "       [-3.10912853, -6.39622925,  0.40903881],\n",
       "       [-1.38837568, -3.58748659,  0.06696106],\n",
       "       [-3.24761429, -6.50154588,  0.43911075],\n",
       "       [-1.20736645, -2.84631835,  0.11627753],\n",
       "       [-3.29866725, -6.51381269,  0.43071923],\n",
       "       [-1.2842916 , -3.25985473,  0.06312332],\n",
       "       [-1.27873797, -3.24697992,  0.20197484],\n",
       "       [-2.15878278, -4.60326233,  0.22110641],\n",
       "       [-3.52804361, -6.85561758,  0.41281609],\n",
       "       [-1.3398886 , -3.3098612 ,  0.08318737],\n",
       "       [-2.98335161, -5.62849284,  0.36477999],\n",
       "       [-1.38249121, -3.39662894,  0.09143691],\n",
       "       [-3.35518252, -7.13004384,  0.49185875],\n",
       "       [-2.59464762, -5.34337337,  0.29034108],\n",
       "       [-2.66653319, -5.8479363 ,  0.38052185],\n",
       "       [-3.02416677, -6.47681517,  0.32281773],\n",
       "       [-2.83250428, -6.18941477,  0.33525254],\n",
       "       [-2.77103502, -5.9309999 ,  0.29435791],\n",
       "       [-2.53468272, -5.36224367,  0.26423932],\n",
       "       [-1.21630238, -2.91102599,  0.08579563],\n",
       "       [-2.46234807, -5.26881262,  0.2900828 ],\n",
       "       [-2.5578253 , -5.07021368,  0.21163564],\n",
       "       [-1.37103005, -3.31139186,  0.07179439],\n",
       "       [-2.79566452, -5.79201335,  0.34425003],\n",
       "       [-1.2694659 , -3.34445304,  0.07436913],\n",
       "       [-2.35379728, -5.13562132,  0.22314645],\n",
       "       [-2.22252649, -4.52931318,  0.15906764],\n",
       "       [-3.6050601 , -6.70251934,  0.4019934 ],\n",
       "       [-2.65595739, -6.02587488,  0.32429134],\n",
       "       [-3.10056385, -6.43118514,  0.36090313],\n",
       "       [-3.38752003, -7.42334007,  0.49865445],\n",
       "       [-2.56346721, -5.32354353,  0.31198066],\n",
       "       [-2.97045501, -5.86300721,  0.39955366],\n",
       "       [-2.88769478, -6.17772627,  0.41241317],\n",
       "       [-1.37138844, -3.42821743,  0.08189244]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 4\n",
    "neurons = [16, 3]\n",
    "activations = [\"relu\", \"identity\"]\n",
    "net = MultiLayerNetwork(input_dim, neurons, activations)\n",
    "dat = np.loadtxt(\"iris.dat\")\n",
    "np.random.shuffle(dat)\n",
    "x = dat[:, :4]\n",
    "y = dat[:, 4:]\n",
    "split_idx = int(0.8 * len(x))\n",
    "x_train = x[:split_idx]\n",
    "y_train = y[:split_idx]\n",
    "x_val = x[split_idx:]\n",
    "y_val = y[split_idx:]\n",
    "\n",
    "net.forward(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "neurons = [16, 3]\n",
    "activations = [\"relu\", \"identity\"]\n",
    "net = MultiLayerNetwork(input_dim, neurons, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<__main__.LinearLayer object at 0x7f334a3d47c0>,\n",
       "       <__main__.ReluLayer object at 0x7f334a3da490>,\n",
       "       <__main__.LinearLayer object at 0x7f334a3da940>, 'identity'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
